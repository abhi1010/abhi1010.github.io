<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algo on Coders Digest</title>
    <link>http://abhipandey.com/tags/algo/</link>
    <description>Recent content in Algo on Coders Digest</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Jun 2014 15:02:07 +0000</lastBuildDate>
    
	<atom:link href="http://abhipandey.com/tags/algo/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Partition Linked List around a Value X</title>
      <link>http://abhipandey.com/2014/06/partition-linked-list-around-a-value-x/</link>
      <pubDate>Mon, 09 Jun 2014 15:02:07 +0000</pubDate>
      
      <guid>http://abhipandey.com/2014/06/partition-linked-list-around-a-value-x/</guid>
      <description>How do you partition a list around a value x, such that all nodes less than x come before all nodes greater than or equal to x?
Well, there are some solutions possible. The solution, I came up with, is a bit convoluted but let me tell the idea behind it. You want to track the following:
 Two pointers to remember the beginning of the lower and higher series each</description>
    </item>
    
    <item>
      <title>Find the Kth to Last Element of a Singly Linked List</title>
      <link>http://abhipandey.com/2014/06/find-the-kth-to-last-element-of-a-singly-linked-list/</link>
      <pubDate>Sun, 08 Jun 2014 07:49:47 +0000</pubDate>
      
      <guid>http://abhipandey.com/2014/06/find-the-kth-to-last-element-of-a-singly-linked-list/</guid>
      <description>It is possible to a recursive solutions but I will use a simple runner logic. Recursive solutions are usually less optimal.
Note here that, in our logic K=1 would return the last element in the linked list. Similarly, K=2 would return the second last element.
The suggested solution here is to use two pointers:
 One pointer will first travel K items into the list Once that is done, both the pointers start travelling together, one item at a time They keep travelling until the end of linked list is found In that situation, the first pointer is at the end of the list, but the second pointer would have only reached till Kth element - this is what you want  Let&amp;rsquo;s have a look at the code:</description>
    </item>
    
    <item>
      <title>Removing Duplicates from Linked List</title>
      <link>http://abhipandey.com/2014/06/removing-duplicates-from-linked-list/</link>
      <pubDate>Sat, 07 Jun 2014 16:50:12 +0000</pubDate>
      
      <guid>http://abhipandey.com/2014/06/removing-duplicates-from-linked-list/</guid>
      <description>Duplicates can be removed in many ways:
 Create a new Linked List containing only unique items
 Iterate through the Linked List and keep removing items that are being repeated
  The internal structure itself for the algo can either be map or set based. When using map the Node itself can be saved thereby making your life easier if you are creating a new Linked List. However sets can be very useful if we are just iterating through the Linked List and simply deleting items that are being repetetive.</description>
    </item>
    
    <item>
      <title>Deleting a Node from Singly Linked List</title>
      <link>http://abhipandey.com/2014/06/deleting-a-node-from-singly-linked-list/</link>
      <pubDate>Sat, 07 Jun 2014 10:49:17 +0000</pubDate>
      
      <guid>http://abhipandey.com/2014/06/deleting-a-node-from-singly-linked-list/</guid>
      <description>Deleting a Node from Singly Linked List is rather straight forward.
 You have to know the head first of all
 Start by checking the head if that&amp;rsquo;s the one you are looking for
 Keep moving forward and checking - always check for null pointers everywhere
  Before we talk about the code, let&amp;rsquo;s see how Linked List is setup.  Now, below is the code for it&amp;hellip;.</description>
    </item>
    
    <item>
      <title>Heap Sort vs Merge Sort vs Insertion Sort vs Radix Sort vs Counting Sort vs Quick Sort</title>
      <link>http://abhipandey.com/2014/03/heap-sort-vs-merge-sort-vs-insertion-sort-vs-radix-sort-vs-counting-sort-vs-quick-sort/</link>
      <pubDate>Wed, 19 Mar 2014 05:42:00 +0000</pubDate>
      
      <guid>http://abhipandey.com/2014/03/heap-sort-vs-merge-sort-vs-insertion-sort-vs-radix-sort-vs-counting-sort-vs-quick-sort/</guid>
      <description>I had written about sorting algorithms (Tag: Sorting) with details about what to look out for along with their code snippets but I wanted a do a quick comparison of all the algos together to see how do they perform when the same set of input is provided to them. Hence I started working on a simple implementation for each one of them. I have now put together all of them in a single project on GitHub.</description>
    </item>
    
    <item>
      <title>Heap Sort</title>
      <link>http://abhipandey.com/2012/10/heap-sort/</link>
      <pubDate>Wed, 17 Oct 2012 14:15:00 +0000</pubDate>
      
      <guid>http://abhipandey.com/2012/10/heap-sort/</guid>
      <description>Heap Sort algo has the following properties:
 The top element (root) is always the next in order
 This allows you to remove one element at a time (the root) and ensure that you are pulling out items in a sorted order
 Always takes O(n*log(n)) time - worst case or best case
  Pros and cons to both   Simple implementations require additional space to hold heap of size n</description>
    </item>
    
    <item>
      <title>Merge Sort</title>
      <link>http://abhipandey.com/2012/10/merge-sort/</link>
      <pubDate>Sat, 06 Oct 2012 14:26:00 +0000</pubDate>
      
      <guid>http://abhipandey.com/2012/10/merge-sort/</guid>
      <description>Merge Sort
 Complexity is O(n log n) Needs more space to merge - proportional to the size of the array Stable Sort * Preserves the order of equal elements Merge Sort does about 39% lower comparisons, in worst case, compared to Quicksort&amp;rsquo;s average case The algo almost always behaves in the same way; taking relatively the same amount of time, whether sorted or unsorted arrays   Testing Notes  Started testing the algo with two versions.</description>
    </item>
    
    <item>
      <title>Quick Sort</title>
      <link>http://abhipandey.com/2012/09/quick-sort/</link>
      <pubDate>Sat, 22 Sep 2012 14:24:00 +0000</pubDate>
      
      <guid>http://abhipandey.com/2012/09/quick-sort/</guid>
      <description>Quick Sort is an efficient divide and conquer algorithm performed in two phases - partition and sorting phase.
Here are few pointers to remember about Quick Sort:
 Partitioning places all the elements less than the pivot in the left part of the array and greater elements in the right part Pivot element stays in its place After partitioning no element moves to the other side, of the pivot * This allows you to sort the elements, to the left or right of the pivot, independent of the other side Complexity is O(n log n) Often fast for small arrays with a few distinct values, repeated many times It is a conquer-and-divide algo; with most of the work happening during partitioning phase If you had to choose the optimum pivot then it should the median of the given array Not a stable sort  Testing Notes  Currently we have only one version of the code.</description>
    </item>
    
    <item>
      <title>Insertion Sort</title>
      <link>http://abhipandey.com/2012/09/insertion-sort/</link>
      <pubDate>Mon, 17 Sep 2012 16:00:00 +0000</pubDate>
      
      <guid>http://abhipandey.com/2012/09/insertion-sort/</guid>
      <description>Insertion Sort has the following properties:
 It works by moving elements one at a time Works really well for small data sets Consider going with this when the input data may already be sorted or partially sorted The may not have to move the elements around, thereby saving precious cycles Stable sort Keeps the original order of elements with equal values  Testing Notes  Had a very interesting time testing my code.</description>
    </item>
    
    <item>
      <title>Radix Sort</title>
      <link>http://abhipandey.com/2012/09/radix-sort/</link>
      <pubDate>Thu, 13 Sep 2012 10:37:00 +0000</pubDate>
      
      <guid>http://abhipandey.com/2012/09/radix-sort/</guid>
      <description>It is a non-comparative integer sorting algorithm. It sorts data by grouping keys by the individual digits which share the same significant position and value. Think Tens, Hundreds, Thousands etc. Some pointers about Radix Sort:
 Even though it is an integer sorting algorithm, it is not restricted just to integers. Integers can also represent strings of characters Two types of radix sort are:  LSD (Least Significant Digit): Short keys come before long keys MSD (Most Significant Digit) Sorting: Lexicographic Order.</description>
    </item>
    
    <item>
      <title>Counting Sort</title>
      <link>http://abhipandey.com/2012/09/counting-sort/</link>
      <pubDate>Tue, 11 Sep 2012 10:03:00 +0000</pubDate>
      
      <guid>http://abhipandey.com/2012/09/counting-sort/</guid>
      <description>Counting Sort is an integer sorting algorithm. It is not very famous when somebody talks about sorting algorithms but it is great when sorting integers. In fact, many a times it may even beat other Sorting Algorithms. The highlight of Counting Sort is that it creates a bucket array (to keep track of frequency of numbers) whose size is the maximum element in the provided array.
We are looking to compare most of the sorting algorithms to find out which one performs better under different circumstances.</description>
    </item>
    
  </channel>
</rss>